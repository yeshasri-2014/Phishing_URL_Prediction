{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netaddr import valid_ipv4\n",
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "from dateutil.parser import parse as date_parse\n",
    "from urllib.parse import urlparse\n",
    "import pythonwhois\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "from googlesearch import search\n",
    "\n",
    "%run ssl_check.ipynb\n",
    "%run pyphishtank.ipynb\n",
    "\n",
    "# Calculates number of months\n",
    "def diff_month(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "# Generate data set by extracting the features from the URL\n",
    "def generate_data_set(url):\n",
    "\n",
    "    data_set = []\n",
    "    \n",
    "    o = urlparse(url)\n",
    "    print(o)\n",
    "    \n",
    "    # Stores the response of the given URL\n",
    "    try:\n",
    "        response = requests.get(url,verify=False)\n",
    "    except:\n",
    "        response = \"\"\n",
    "    #print(response.text)\n",
    "          \n",
    "    #If URL has IP address set 1 else -1\n",
    "    if (valid_ipv4(o.netloc.split(\":\")[0])):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)    \n",
    "    \n",
    "    # URL_Length\n",
    "    if len(url) < 54:\n",
    "        data_set.append(-1)\n",
    "    elif len(url) >= 54 and len(url) <= 75:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "        \n",
    "    # Shortining_Service\n",
    "    if re.findall(\"goo.gl|bit.ly\", url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "    \n",
    "    # having_At_Symbol\n",
    "    if re.findall(\"@\", url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "    \n",
    "    # double_slash_redirecting\n",
    "    if re.findall(r\"[^https?:]//\",url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "      \n",
    "    # Prefix_Suffix\n",
    "    if re.findall(r\"-\", url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # having_Sub_Domain - Need to work on this\n",
    "    if len(re.findall(\"\\.\", url)) == 1:\n",
    "        data_set.append(-1)\n",
    "    elif len(re.findall(\"\\.\", url)) == 2:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "      \n",
    "    \n",
    "    #USING HTTPS\n",
    "    ca_test=0\n",
    "    ca_valid=0\n",
    "    \n",
    "    valid_ca=[\"GeoTrust\", \"GoDaddy\", \"Network Solutions\", \"Thawte\", \"Comodo\", \"Doster\",\"VeriSign\"]\n",
    "    primary_uri=o.netloc.split(\":\")\n",
    "    try:\n",
    "        #check for port number\n",
    "        if (o.scheme == \"http\"):\n",
    "            data_set.append(1)\n",
    "        elif (o.scheme == \"https\"):\n",
    "            port=443\n",
    "            \n",
    "            #Get the primary uri and see if we have port in it ex: https://testphish.com:5454\n",
    "            #primary_uri=o.netloc.split(\":\")\n",
    "            if (len(primary_uri) > 1):\n",
    "                port = primary_uri[1]\n",
    "           \n",
    "            get_cert_data=print_basic_info(get_certificate(primary_uri[0],port))\n",
    "            #print(get_cert_data)\n",
    "    \n",
    "            #Check whether CA is whitelisted one\n",
    "            for ca in valid_ca:\n",
    "                if (ca.lower() in get_cert_data[\"issuer\"].lower()):\n",
    "                    ca_test=1\n",
    "    \n",
    "            if (diff_month(get_cert_data[\"notafter\"],date.today()) >= 12):\n",
    "                ca_valid=1\n",
    "    \n",
    "            #Use https and Issuer Is Trusted &and Age of Certificate≥ 1 Years\n",
    "            if (o.scheme == \"https\" and ca_test == 1 and ca_valid == 1):\n",
    "                data_set.append(-1)\n",
    "        \n",
    "            #Using https and Issuer Is Not Trusted  → Suspicious\n",
    "            #elif(o.scheme == \"https\" and ca_test == 0):\n",
    "            #    data_set.append(0)\n",
    "            #Otherwise→ Phishing\n",
    "            else:\n",
    "                data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(0)\n",
    "        \n",
    "    # Domain_registeration_length   \n",
    "    domain=primary_uri[0]\n",
    "    details=pythonwhois.get_whois(domain)\n",
    "    isDNSvalid=1\n",
    "    \n",
    "    try:\n",
    "        if (details['status'][0] == \"invalid\"):\n",
    "            data_set.append(1)\n",
    "            isDNSvalid=0\n",
    "    except:\n",
    "        isDNSvalid=1\n",
    "            \n",
    "    try:    \n",
    "        if (isDNSvalid != 0):\n",
    "            #Domains Expires in≤ 1 years → Phishing\n",
    "            if (diff_month(details[\"expiration_date\"][0],date.today()) <= 12):\n",
    "                data_set.append(1)\n",
    "                #Otherwise→ Legitimate\n",
    "            else:\n",
    "                data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "        \n",
    "    #Favicon\n",
    "    data_set.append(2)\n",
    "    \n",
    "    #port\n",
    "    data_set.append(-1)\n",
    "    \n",
    "    #Existence of “HTTPS” Token in the Domain Part of the URL\n",
    "    if (\"https\" in primary_uri[0]):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "        \n",
    "    # Request_URL\n",
    "    data_set.append(-1)\n",
    "\n",
    "    # URL_of_Anchor\n",
    "    data_set.append(-1)\n",
    "\n",
    "    # Links_in_tags\n",
    "    data_set.append(-1)\n",
    "\n",
    "    # SFH\n",
    "    data_set.append(-1)\n",
    "\n",
    "    # Submitting_to_email\n",
    "    try :\n",
    "        if (reponse != \"\" ):\n",
    "            if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
    "                data_set.append(1)\n",
    "            else:\n",
    "                data_set.append(-1)\n",
    "    except: \n",
    "         data_set.append(-1)\n",
    "\n",
    "    # Abnormal_URL\n",
    "    ext = tldextract.extract(url)\n",
    "    try:\n",
    "        #print(ext.domain)\n",
    "        # Requests all the information about the domain, if domain don't exists we will hit exception\n",
    "        whois_response = requests.get(\"https://www.whois.com/whois/\"+ext.domain)\n",
    "        reg_data = re.findall(r'Registered On:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "        if reg_data != \"\":\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # Website Forwarding\n",
    "    try:\n",
    "        if (responseonse != \"\"):\n",
    "            if len(response.history) <= 1:\n",
    "                data_set.append(-1)\n",
    "            elif len(response.history) >=2 and len(response.history) < 4:\n",
    "                data_set.append(0)\n",
    "            else:\n",
    "                data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "     # on_mouseover\n",
    "    try:\n",
    "        if (response != \"\"):        \n",
    "            if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "                data_set.append(1)\n",
    "            else:\n",
    "                data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "        \n",
    "    # RightClick\n",
    "    try:\n",
    "        if (response != \"\"):\n",
    "            if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "                data_set.append(1)\n",
    "            else:\n",
    "                data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # popUpWidnow\n",
    "    #try:\n",
    "    #   if (response != \"\"):\n",
    "    #       if re.findall(r\"alert\\(\", response.text):\n",
    "    #            data_set.append(1)\n",
    "    #        else:\n",
    "    #            data_set.append(-1)\n",
    "    #except:\n",
    "    #    data_set.append(-1)\n",
    "    data_set.append(2)\n",
    "  \n",
    "    #Iframe\n",
    "    #try:\n",
    "    #    if (response != \"\"):\n",
    "    #        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "    #            data_set.append(1)\n",
    "    #        else:\n",
    "    #            data_set.append(-1)\n",
    "    #except:\n",
    "    #    data_set.append(-1)\n",
    "    data_set.append(2)\n",
    "        \n",
    "    try:\n",
    "        if (isDNSvalid == 0):\n",
    "            data_set.append(1)\n",
    "        #Age Of Domain <= 6 months-Phishing\n",
    "        elif (diff_month(date.today(),details[\"creation_date\"][0]) < 6):\n",
    "            data_set.append(1)\n",
    "        #Otherwise→ Legitimate\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "        \n",
    "    try:\n",
    "        if (isDNSvalid == 0):\n",
    "            data_set.append(1)\n",
    "        # DNSRecord\n",
    "        elif (len(details)):\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "        \n",
    "        \n",
    "    #Website Traffic\n",
    "    #(Website Rank<100,000 → Legitimate@Website Rank>100,000 →Suspicious@Otherwise → Phish)\n",
    "    print(primary_uri[0])\n",
    "    xml = requests.get('http://data.alexa.com/data?cli=10&dat=s&url=%s'%url)\n",
    "    try: \n",
    "        rank = int(re.search(r'<POPULARITY[^>]*TEXT=\"(\\d+)\"', xml.text).groups()[0])\n",
    "        if (rank < 100000):\n",
    "            data_set.append(-1)\n",
    "        elif (rank > 100000):\n",
    "            data_set.append(0)\n",
    "    except: \n",
    "        data_set.append(1)\n",
    "        \n",
    "    #page rank\n",
    "    #check_rank=\"https://www.alexa.com/siteinfo/\"+primary_uri[0]\n",
    "    #rank_checker_response = requests.post(check_rank)\n",
    "    page_rank=0\n",
    "    try:\n",
    "        rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\"name\": url})\n",
    "        # Extracts page rank\n",
    "        if (re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text) == []):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            page_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "            #PageRank<0.2 → Phishing\n",
    "            print(page_rank)\n",
    "            if (page_rank > 0.2):\n",
    "                data_set.append(-1)\n",
    "            else:\n",
    "                data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "        \n",
    "    # Google_Index\n",
    "    #https://www.geeksforgeeks.org/performing-google-search-using-python-code/\n",
    "    try:\n",
    "        #Use double quotations to search for exact string\n",
    "        domain=\"\\\"{}\\\"\".format(ext.domain)\n",
    "        subdomain=\"\\\"{}\\\"\".format(ext.subdomain)\n",
    "        \n",
    "        #Use subdomain for the search if present else domain\n",
    "        if (subdomain == \"\"):\n",
    "            search_str=domain\n",
    "        else:\n",
    "            search_str=subdomain\n",
    "          \n",
    "        tld_conf=ext.suffix\n",
    "        #print(search_str,tld_conf,ext)\n",
    "        count = 0\n",
    "        for j in search(search_str, tld=tld_conf, num=2, stop=1,pause=1): \n",
    "            #print(j)\n",
    "            count+=1\n",
    "        #print(count)\n",
    "        if (count != 0):\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # Links_pointing_to_page\n",
    "    try:\n",
    "        if (response != \"\"):\n",
    "            number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "            if number_of_links == 0:\n",
    "                data_set.append(1)\n",
    "            elif number_of_links <= 2:\n",
    "                data_set.append(0)\n",
    "            else:\n",
    "                data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "        \n",
    "    # Statistical_report\n",
    "    p = PhishTank()\n",
    "    result = p.check(url)\n",
    "    print(url, result.valid)\n",
    "    if result.in_database:\n",
    "        if result.valid:\n",
    "            #print(\"{url} is a phish!\".format(url=result.url))\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            #print(\"{url} is not a phish!\".format(url=result.url))\n",
    "            data_set.append(-1)\n",
    "    else:\n",
    "        #print(\"{url} is not in the PhishTank database\".format(url=result.url))\n",
    "        data_set.append(-1)\n",
    "      \n",
    "    print(data_set) \n",
    "    \n",
    "    ds_columns=[\"having_IP_Address\",\"URL_Length\",\"Shortining_Service\",\"having_At_Symbol\",\"double_slash_redirecting\",\"Prefix_Suffix\",\n",
    "            \"having_Sub_Domain\",\"SSLfinal_State\",\"Domain_registeration_length\",\"Favico\",\"port\",\"HTTPS_token\",\"Request_URL\",\n",
    "            \"URL_of_Anchor\",\"Links_in_tags\",\"SFH\",\"Submitting_to_email\",\"Abnormal_URL\",\"Redirect\",\"on_mouseover\",\"RightClick\",\n",
    "            \"popUpWidnow\",\"Iframe\", \"age_of_domain\",\"DNSRecord\",\"web_traffic\",\"Page_Rank\",\"Google_Index\",\n",
    "            \"Links_pointing_to_page\",\"Statistical_report\"]\n",
    "    print(dict(zip(ds_columns, data_set)))\n",
    "    \n",
    "    #Remove \"2\"s as we are not using this features\n",
    "    new_data=[]\n",
    "    for x in data_set:\n",
    "        if (x != 2):\n",
    "            new_data.append(x)\n",
    "    return(new_data)\n",
    "    \n",
    "#print(generate_data_set(\"https://paypal-e.com/auth/main/content/\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
